{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a483fea-5052-486a-a650-5801d6883aa1",
   "metadata": {},
   "source": [
    "# Regresión lineal multivariable: Descenso de gradiente\n",
    "M2U2 - Ejercicio 2\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Implementar la optimización de la función de coste por gradient descent, o lo que es lo mismo, entrenar el modelo\n",
    "\n",
    "Recuerda seguir las instrucciones para las entregas de prácticas indicadas en [Instrucciones entregas](https://github.com/Tokio-School/Machine-Learning/blob/main/Instrucciones%20entregas.md).\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "Este ejercicio es una continuación del ejercicio anterior \"Función de coste\", por lo que debes basarte en el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43af36-a639-4381-8424-72241955e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96c4-774a-4e0b-88a2-9c23c7f9181b",
   "metadata": {},
   "source": [
    "## Tarea 1: Implementar la función de coste para regresión lineal multivariable\n",
    "\n",
    "En esta tarea, debes copiar la celda correspondiente del ejercicio anterior, trayendo tu código para implementar la función de coste vectorizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300d424-c3ba-47d5-9254-65716fbd1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de coste vectorizada siguiendo la siguiente plantilla\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    \n",
    "    Devuelve:\n",
    "    j -- float con el coste para dicho array theta\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recuerda comprobar las dimensiones de la multiplicación matricial para hacerla correctamente\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09977c-81e4-44d3-babf-e4a6ee0a9fb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tarea 2: Implementar la optimización de dicha función de coste por gradient descent\n",
    "\n",
    "Ahora vamos a resolver la optimización de dicha función de coste para entrenar el modelo, mediante el método de gradient descent de forma vectorizada. El modelo se considerará entrenado cuando su función de coste haya alcanzado un valor mínimo y estable.\n",
    "\n",
    "$$Y = h_\\Theta(X) = X \\times \\Theta^T$$\n",
    "\n",
    "$$J_\\theta = \\frac{1}{2m} \\sum_{i = 0}^{m} (h_\\theta(x^i) - y^i)^2$$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i = 0}^{m}{(h_\\theta(x^i) - y^i) x_j^i}]$$\n",
    "\n",
    "Para ello, de nuevo, rellena la plantilla de código de la siguiente celda.\n",
    "\n",
    "Consejos:\n",
    "- Si lo prefieres, puedes implementar primero la función con bucles e iteraciones y por último de forma vectorizada\n",
    "- Recuerda las dimensiones de cada vector/matriz\n",
    "- De nuevo, anota las operaciones por orden paso a paso en una hoja o celda auxiliar\n",
    "- En cada paso, anota las dimensiones de su resultado, que también puedes comprobar en tu código\n",
    "- Usa numpy.matmul() como multiplicación de matrices\n",
    "- Al inicio de cada iteración de entrenamiento, debes copiar toda $\\Theta$, puesto que vas a iterar actualizando cada uno de sus valores basándote en el vector completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0980f-0796-4f79-aed6-d02f9e95d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función que entrena el modelo por gradient descent\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, e, iter_):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    # Su nº máx. de elementos será el nº máx. de iteraciones\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        theta_iter = [...]    # Copia con \"deep copy\" la theta para cada iteración, ya que debemos actualizarla\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter    # Actualiza toda la theta, lista para la siguiente iteración\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e, para toda iteración\n",
    "        # excepto la primera\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b469bd3-80f2-4cbe-baa5-6a10c06f4ca1",
   "metadata": {},
   "source": [
    "## Tarea 3: Comprobar la implementación del gradient descent\n",
    "\n",
    "Para comprobar tu implementación, de nuevo, utiliza la misma celda variando sus parámetros varias veces, representando gráficamente la evolución de la función de coste y viendo cómo su valor va acercándose a 0.\n",
    "\n",
    "En cada caso, comprueba que la $\\Theta$ inicial y final son muy similares en los siguientes escenarios:\n",
    "1. Genera varios datasets sintéticos, comprobando cada uno\n",
    "1. Modifica el nº de ejemplos y características, m y n\n",
    "1. Modifica el parámetro de error, lo que puede hacer que la $\\Theta$ inicial y final no concuerden del todo, y a mayor error más diferencia puede haber\n",
    "1. Comprueba los hiper-parámetros del nº máx. de iteraciones o el ratio de entrenamiento $\\alpha$, que hará que el modelo tarde más o menos en entrenarse, dentro de unos valores mínimos y máximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609719f-aab2-4ea3-94c5-6c05c72deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético, con término de error, de la forma que escojas, con Numpy o Scikit-learn\n",
    "\n",
    "m = 0\n",
    "n = 0\n",
    "e = 0.\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones (forma o \"shape\") de los vectores\n",
    "print('Theta real a estimar:')\n",
    "print()\n",
    "print('shape')\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print('shape', 'shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d7f8c-ef7e-4358-b027-32f0fac388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Utiliza una theta iniciada aleatoriamente o la Theta_verd, en función del escenario a comprobar\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\nCoste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26243d6-8737-417c-acbb-1ae426dd05d0",
   "metadata": {},
   "source": [
    "Representa gráficamente el histórico de la función de coste para comprobar tu implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdecba-cf90-4d35-99b7-5da9cc2b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente la función de coste vs el nº de iteraciones\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('nº iteraciones')\n",
    "plt.ylabel('coste')\n",
    "\n",
    "plt.plot([...])    # Completar\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3a720-c473-4e29-b3c7-0bda5bea5bbb",
   "metadata": {},
   "source": [
    "Para comprobar completamente la implementación de dichas funciones, modifica el dataset sintético original para comprobar que la función de coste y el entrenamiento por gradient descent siguen funcionando correctamente.\n",
    "\n",
    "P. ej., modifica el nº de ejemplos y el nº de características.\n",
    "\n",
    "También añádele de nuevo un término de error a la Y. En este caso, puede que la Theta inicial y la final no concuerden del todo, ya que hemos introducido error o \"ruido\" en el dataset de entrenamiento.\n",
    "\n",
    "Por último, comprueba todos los hiper-parámetros de tu implementación. Utiliza varios valores de alpha, e, nº de iteraciones, etc., y comprueba que los resultados son los esperados."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
